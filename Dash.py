# -*- coding: utf-8 -*-
"""classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bxJcMWKI-iRBlDU2SdTBBSZb_ASbw48x
"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score

import pandas as pd
import numpy as np
df = pd.read_csv('/content/CaseStudy_FraudIdentification.csv')
df.columns

"""check if there are any categorical columns or all columns are numerical"""

df.info()

"""Here we can clearly see that "Gender","EDUCATION" and "MARRIAGE" are categorical in nature , we need to convert them to numerical

Before doing that lets check if there are any numerical data in categorical field , if yes then replace it with mode
"""

# Find categorical columns
categorical_columns = df.select_dtypes(include=['object']).columns

# Find percentage of numerical data in categorical columns
for col in categorical_columns:
    num_data = pd.to_numeric(df[col], errors='coerce').notnull().sum()
    percent_num_data = num_data / len(df[col]) * 100
    print("Percentage of numerical data in column {}: {:.2f}%".format(col, percent_num_data))

"""here we can see that Marriage column has 0.18 percent data numerical in nature , lets replace with its mode."""

for col in categorical_columns:
    if df[col].dtype == np.object:
        mode = df[col].mode().values[0]
        df[col] = df[col].replace('0', mode)

"""lets confirm if there are any numerical values in categorical columns."""

# Find percentage of numerical data in categorical columns
for col in categorical_columns:
    num_data = pd.to_numeric(df[col], errors='coerce').notnull().sum()
    percent_num_data = num_data / len(df[col]) * 100
    print("Percentage of numerical data in column {}: {:.2f}%".format(col, percent_num_data))

"""Now we can clearly see there are  no numerical values in categorical columns.

lets see the data stats to have some insights
"""

df.describe()

"""we can see that data in some columns ranges from low values to very high values , lets scale them"""

df.columns

# define a list of column names to scale
columns_to_scale = ['LIMIT_BAL','BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3','BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6','PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3','PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']

# scale the specified columns
scaler = StandardScaler()
df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])

"""Now lets see the stats of our data"""

df.describe()

"""Lets look if our data still has any missing values"""

# find missing values

# Check for missing values in each column
missing_values_count = df.isnull().sum()

# Print the number of missing values in each column
print(missing_values_count)

# Calculate the percentage of missing values in each column
total_cells = np.product(df.shape)
total_missing = missing_values_count.sum()
percent_missing = (total_missing / total_cells) * 100
print("Percentage of missing values: {:.2f}%".format(percent_missing))

"""our data do no have any missing values , lets convert categorical features in numerical"""

#lets see which of the columns are categorical
df.info()

"""there are three columns , which are categorical in nature, lets convert them into numerical"""

# Convert categorical variables to numeric using one-hot encoding
df = pd.get_dummies(df, columns=['EDUCATION', 'MARRIAGE','Gender'], prefix=['edu', 'mar','gen'])

"""lets verify if all columns are categorical or not"""

df.info()

# Separate features and target variable
X = df.drop('default payment next month', axis=1)
y = df['default payment next month']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize LightGBM classifier model
lgb_model = lgb.LGBMClassifier(learning_rate=0.1,max_depth=-6,random_state=42)

# Train model on training data
lgb_model.fit(X_train,y_train,eval_set=[(X_test,y_test),(X_train,y_train)],
          verbose=20,eval_metric='logloss')


y_pred = lgb_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
print(accuracy,precision,recall)

import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score

# load data
#data = pd.read_csv('/content/CaseStudy_FraudIdentification.csv')


# split data into training and testing sets

X = df.drop('default payment next month', axis=1)
y = df['default payment next month']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# define LightGBM classifier and set parameters
clf = lgb.LGBMClassifier(boosting_type='gbdt', learning_rate=0.1, n_estimators=100)

# set parameter grid for hyperparameter search
param_grid = {
    'num_leaves': [31, 50, 100],
    'max_depth': [-1, 5, 10],
    'feature_fraction': [0.5, 0.8, 1.0],
    'min_child_samples': [10, 20, 30],
    'reg_alpha': [0.0, 0.1, 0.5],
    'reg_lambda': [0.0, 0.1, 0.5],
}

# perform grid search with cross-validation
grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')
grid_search.fit(X_train, y_train)

# print the best hyperparameters
print("Best hyperparameters:", grid_search.best_params_)

# make predictions on testing set using the best estimator
best_clf = grid_search.best_estimator_
y_pred = best_clf.predict(X_test)

# evaluate performance with accuracy and precision scores
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
print('Accuracy:', accuracy)
print('Precision:', precision)


